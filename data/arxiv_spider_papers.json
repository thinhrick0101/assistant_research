[
  {
    "paper_id": "0007125v1",
    "title": "On The Homflypt Skein Module of S^1 x S^2",
    "authors": [
      "Patrick M. Gilmer",
      "Jianyuan Zhong"
    ],
    "abstract": "Let $k$ be a subring of the field of rational functions in $x, v, s$ which\ncontains $x^{\\pm 1}, v^{\\pm 1}, s^{\\pm 1}$. If $M$ is an oriented 3-manifold,\nlet $S(M)$ denote the Homflypt skein module of $M$ over $k$. This is the free\n$k$-module generated by isotopy classes of framed oriented links in $M$\nquotiented by the Homflypt skein relations: (1)\n$x^{-1}L_{+}-xL_{-}=(s-s^{-1})L_{0}$; (2) $L$ with a positive twist\n$=(xv^{-1})L$; (3) $L\\sqcup O=(\\frac{v-v^{-1}}{s-s^{-1}})L$ where $O$ is the\nunknot. We give two bases for the relative Homflypt skein module of the solid\ntorus with 2 points in the boundary. The first basis is related to the basis of\n$S(S^1\\times D^2)$ given by V. Turaev and also J. Hoste and M. Kidwell; the\nsecond basis is related to a Young idempotent basis for $S(S^1\\times D^2)$\nbased on the work of A. Aiston, H. Morton and C. Blanchet. We prove that if the\nelements $s^{2n}-1$, for $n$ a nonzero integer, and the elements\n$s^{2m}-v^{2}$, for any integer $m$, are invertible in $k$, then $S(S^{1}\n\\times S^2)=k$-torsion module $\\oplus k$. Here the free part is generated by\nthe empty link $\\phi$. In addition, if the elements $s^{2m}-v^{4}$, for $m$ an\ninteger, are invertible in $k$, then $S(S^{1} \\times S^2)$ has no torsion. We\nalso obtain some results for more general $k$.",
    "url": "http://arxiv.org/abs/math/0007125v1",
    "pdf_url": "http://arxiv.org/pdf/math/0007125v1",
    "published_date": "2000-07-20",
    "source": "arxiv",
    "categories": [
      "math.GT",
      "math.QA",
      "57M99"
    ]
  },
  {
    "paper_id": "2010.13154v2",
    "title": "Attention is All You Need in Speech Separation",
    "authors": [
      "Cem Subakan",
      "Mirco Ravanelli",
      "Samuele Cornell",
      "Mirko Bronzi",
      "Jianyuan Zhong"
    ],
    "abstract": "Recurrent Neural Networks (RNNs) have long been the dominant architecture in\nsequence-to-sequence learning. RNNs, however, are inherently sequential models\nthat do not allow parallelization of their computations. Transformers are\nemerging as a natural alternative to standard RNNs, replacing recurrent\ncomputations with a multi-head attention mechanism. In this paper, we propose\nthe SepFormer, a novel RNN-free Transformer-based neural network for speech\nseparation. The SepFormer learns short and long-term dependencies with a\nmulti-scale approach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It\nreaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on\nWSJ0-3mix. The SepFormer inherits the parallelization advantages of\nTransformers and achieves a competitive performance even when downsampling the\nencoded representation by a factor of 8. It is thus significantly faster and it\nis less memory-demanding than the latest speech separation systems with\ncomparable performance.",
    "url": "http://arxiv.org/abs/2010.13154v2",
    "pdf_url": "http://arxiv.org/pdf/2010.13154v2",
    "published_date": "2020-10-25",
    "source": "arxiv",
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD",
      "eess.SP"
    ]
  },
  {
    "paper_id": "2403.01446v2",
    "title": "GuardT2I: Defending Text-to-Image Models from Adversarial Prompts",
    "authors": [
      "Yijun Yang",
      "Ruiyuan Gao",
      "Xiao Yang",
      "Jianyuan Zhong",
      "Qiang Xu"
    ],
    "abstract": "Recent advancements in Text-to-Image (T2I) models have raised significant\nsafety concerns about their potential misuse for generating inappropriate or\nNot-Safe-For-Work (NSFW) contents, despite existing countermeasures such as\nNSFW classifiers or model fine-tuning for inappropriate concept removal.\nAddressing this challenge, our study unveils GuardT2I, a novel moderation\nframework that adopts a generative approach to enhance T2I models' robustness\nagainst adversarial prompts. Instead of making a binary classification,\nGuardT2I utilizes a Large Language Model (LLM) to conditionally transform text\nguidance embeddings within the T2I models into natural language for effective\nadversarial prompt detection, without compromising the models' inherent\nperformance. Our extensive experiments reveal that GuardT2I outperforms leading\ncommercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a\nsignificant margin across diverse adversarial scenarios. Our framework is\navailable at https://github.com/cure-lab/GuardT2I.",
    "url": "http://arxiv.org/abs/2403.01446v2",
    "pdf_url": "http://arxiv.org/pdf/2403.01446v2",
    "published_date": "2024-03-03",
    "source": "arxiv",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "paper_id": "2405.13522v2",
    "title": "Beyond Trend and Periodicity: Guiding Time Series Forecasting with Textual Cues",
    "authors": [
      "Zhijian Xu",
      "Yuxuan Bian",
      "Jianyuan Zhong",
      "Xiangyu Wen",
      "Qiang Xu"
    ],
    "abstract": "This work introduces a novel Text-Guided Time Series Forecasting (TGTSF)\ntask. By integrating textual cues, such as channel descriptions and dynamic\nnews, TGTSF addresses the critical limitations of traditional methods that rely\npurely on historical data. To support this task, we propose TGForecaster, a\nrobust baseline model that fuses textual cues and time series data using\ncross-attention mechanisms. We then present four meticulously curated benchmark\ndatasets to validate the proposed framework, ranging from simple periodic data\nto complex, event-driven fluctuations. Our comprehensive evaluations\ndemonstrate that TGForecaster consistently achieves state-of-the-art\nperformance, highlighting the transformative potential of incorporating textual\ninformation into time series forecasting. This work not only pioneers a novel\nforecasting task but also establishes a new benchmark for future research,\ndriving advancements in multimodal data integration for time series models.",
    "url": "http://arxiv.org/abs/2405.13522v2",
    "pdf_url": "http://arxiv.org/pdf/2405.13522v2",
    "published_date": "2024-05-22",
    "source": "arxiv",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "paper_id": "2502.11157v1",
    "title": "Dyve: Thinking Fast and Slow for Dynamic Process Verification",
    "authors": [
      "Jianyuan Zhong",
      "Zeju Li",
      "Zhijian Xu",
      "Xiangyu Wen",
      "Qiang Xu"
    ],
    "abstract": "We present Dyve, a dynamic process verifier that enhances reasoning error\ndetection in large language models by integrating fast and slow thinking,\ninspired by Kahneman's Systems Theory. Dyve adaptively applies immediate\ntoken-level confirmation System 1 for straightforward steps and comprehensive\nanalysis System 2 for complex ones. Leveraging a novel step-wise\nconsensus-filtered process supervision technique, combining Monte Carlo\nestimation with LLM based evaluation, Dyve curates high-quality supervision\nsignals from noisy data. Experimental results on ProcessBench and the MATH\ndataset confirm that Dyve significantly outperforms existing process-based\nverifiers and boosts performance in Best-of-N settings.",
    "url": "http://arxiv.org/abs/2502.11157v1",
    "pdf_url": "http://arxiv.org/pdf/2502.11157v1",
    "published_date": "2025-02-16",
    "source": "arxiv",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "paper_id": "2001.09239v2",
    "title": "Multi-task self-supervised learning for Robust Speech Recognition",
    "authors": [
      "Mirco Ravanelli",
      "Jianyuan Zhong",
      "Santiago Pascual",
      "Pawel Swietojanski",
      "Joao Monteiro",
      "Jan Trmal",
      "Yoshua Bengio"
    ],
    "abstract": "Despite the growing interest in unsupervised learning, extracting meaningful\nknowledge from unlabelled audio remains an open challenge. To take a step in\nthis direction, we recently proposed a problem-agnostic speech encoder (PASE),\nthat combines a convolutional encoder followed by multiple neural networks,\ncalled workers, tasked to solve self-supervised problems (i.e., ones that do\nnot require manual annotations as ground truth). PASE was shown to capture\nrelevant speech information, including speaker voice-print and phonemes. This\npaper proposes PASE+, an improved version of PASE for robust speech recognition\nin noisy and reverberant environments. To this end, we employ an online speech\ndistortion module, that contaminates the input signals with a variety of random\ndisturbances. We then propose a revised encoder that better learns short- and\nlong-term speech dynamics with an efficient combination of recurrent and\nconvolutional networks. Finally, we refine the set of workers used in\nself-supervision to encourage better cooperation. Results on TIMIT, DIRHA and\nCHiME-5 show that PASE+ significantly outperforms both the previous version of\nPASE as well as common acoustic features. Interestingly, PASE+ learns\ntransferable representations suitable for highly mismatched acoustic\nconditions.",
    "url": "http://arxiv.org/abs/2001.09239v2",
    "pdf_url": "http://arxiv.org/pdf/2001.09239v2",
    "published_date": "2020-01-25",
    "source": "arxiv",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ]
  },
  {
    "paper_id": "2405.15833v1",
    "title": "DSPO: An End-to-End Framework for Direct Sorted Portfolio Construction",
    "authors": [
      "Jianyuan Zhong",
      "Zhijian Xu",
      "Saizhuo Wang",
      "Xiangyu Wen",
      "Jian Guo",
      "Qiang Xu"
    ],
    "abstract": "In quantitative investment, constructing characteristic-sorted portfolios is\na crucial strategy for asset allocation. Traditional methods transform raw\nstock data of varying frequencies into predictive characteristic factors for\nasset sorting, often requiring extensive manual design and misalignment between\nprediction and optimization goals. To address these challenges, we introduce\nDirect Sorted Portfolio Optimization (DSPO), an innovative end-to-end framework\nthat efficiently processes raw stock data to construct sorted portfolios\ndirectly. DSPO's neural network architecture seamlessly transitions stock data\nfrom input to output while effectively modeling the intra-dependency of\ntime-steps and inter-dependency among all tradable stocks. Additionally, we\nincorporate a novel Monotonical Logistic Regression loss, which directly\nmaximizes the likelihood of constructing optimal sorted portfolios. To the best\nof our knowledge, DSPO is the first method capable of handling market\ncross-sections with thousands of tradable stocks fully end-to-end from raw\nmulti-frequency data. Empirical results demonstrate DSPO's effectiveness,\nyielding a RankIC of 10.12\\% and an accumulated return of 121.94\\% on the New\nYork Stock Exchange in 2023-2024, and a RankIC of 9.11\\% with a return of\n108.74\\% in other markets during 2021-2022.",
    "url": "http://arxiv.org/abs/2405.15833v1",
    "pdf_url": "http://arxiv.org/pdf/2405.15833v1",
    "published_date": "2024-05-24",
    "source": "arxiv",
    "categories": [
      "q-fin.PM",
      "q-fin.CP"
    ]
  },
  {
    "paper_id": "2407.11095v1",
    "title": "DeepGate3: Towards Scalable Circuit Representation Learning",
    "authors": [
      "Zhengyuan Shi",
      "Ziyang Zheng",
      "Sadaf Khan",
      "Jianyuan Zhong",
      "Min Li",
      "Qiang Xu"
    ],
    "abstract": "Circuit representation learning has shown promising results in advancing the\nfield of Electronic Design Automation (EDA). Existing models, such as DeepGate\nFamily, primarily utilize Graph Neural Networks (GNNs) to encode circuit\nnetlists into gate-level embeddings. However, the scalability of GNN-based\nmodels is fundamentally constrained by architectural limitations, impacting\ntheir ability to generalize across diverse and complex circuit designs. To\naddress these challenges, we introduce DeepGate3, an enhanced architecture that\nintegrates Transformer modules following the initial GNN processing. This novel\narchitecture not only retains the robust gate-level representation capabilities\nof its predecessor, DeepGate2, but also enhances them with the ability to model\nsubcircuits through a novel pooling transformer mechanism. DeepGate3 is further\nrefined with multiple innovative supervision tasks, significantly enhancing its\nlearning process and enabling superior representation of both gate-level and\nsubcircuit structures. Our experiments demonstrate marked improvements in\nscalability and generalizability over traditional GNN-based approaches,\nestablishing a significant step forward in circuit representation learning\ntechnology.",
    "url": "http://arxiv.org/abs/2407.11095v1",
    "pdf_url": "http://arxiv.org/pdf/2407.11095v1",
    "published_date": "2024-07-15",
    "source": "arxiv",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "paper_id": "2502.01681v2",
    "title": "DeepGate4: Efficient and Effective Representation Learning for Circuit Design at Scale",
    "authors": [
      "Ziyang Zheng",
      "Shan Huang",
      "Jianyuan Zhong",
      "Zhengyuan Shi",
      "Guohao Dai",
      "Ningyi Xu",
      "Qiang Xu"
    ],
    "abstract": "Circuit representation learning has become pivotal in electronic design\nautomation, enabling critical tasks such as testability analysis, logic\nreasoning, power estimation, and SAT solving. However, existing models face\nsignificant challenges in scaling to large circuits due to limitations like\nover-squashing in graph neural networks and the quadratic complexity of\ntransformer-based models. To address these issues, we introduce DeepGate4, a\nscalable and efficient graph transformer specifically designed for large-scale\ncircuits. DeepGate4 incorporates several key innovations: (1) an update\nstrategy tailored for circuit graphs, which reduce memory complexity to\nsub-linear and is adaptable to any graph transformer; (2) a GAT-based sparse\ntransformer with global and local structural encodings for AIGs; and (3) an\ninference acceleration CUDA kernel that fully exploit the unique sparsity\npatterns of AIGs. Our extensive experiments on the ITC99 and EPFL benchmarks\nshow that DeepGate4 significantly surpasses state-of-the-art methods, achieving\n15.5% and 31.1% performance improvements over the next-best models.\nFurthermore, the Fused-DeepGate4 variant reduces runtime by 35.1% and memory\nusage by 46.8%, making it highly efficient for large-scale circuit analysis.\nThese results demonstrate the potential of DeepGate4 to handle complex EDA\ntasks while offering superior scalability and efficiency.",
    "url": "http://arxiv.org/abs/2502.01681v2",
    "pdf_url": "http://arxiv.org/pdf/2502.01681v2",
    "published_date": "2025-02-02",
    "source": "arxiv",
    "categories": [
      "cs.LG",
      "cs.AR"
    ]
  },
  {
    "paper_id": "1904.06618v1",
    "title": "UR-FUNNY: A Multimodal Language Dataset for Understanding Humor",
    "authors": [
      "Md Kamrul Hasan",
      "Wasifur Rahman",
      "Amir Zadeh",
      "Jianyuan Zhong",
      "Md Iftekhar Tanveer",
      "Louis-Philippe Morency",
      "Mohammed",
      "Hoque"
    ],
    "abstract": "Humor is a unique and creative communicative behavior displayed during social\ninteractions. It is produced in a multimodal manner, through the usage of words\n(text), gestures (vision) and prosodic cues (acoustic). Understanding humor\nfrom these three modalities falls within boundaries of multimodal language; a\nrecent research trend in natural language processing that models natural\nlanguage as it happens in face-to-face communication. Although humor detection\nis an established research area in NLP, in a multimodal context it is an\nunderstudied area. This paper presents a diverse multimodal dataset, called\nUR-FUNNY, to open the door to understanding multimodal language used in\nexpressing humor. The dataset and accompanying studies, present a framework in\nmultimodal humor detection for the natural language processing community.\nUR-FUNNY is publicly available for research.",
    "url": "http://arxiv.org/abs/1904.06618v1",
    "pdf_url": "http://arxiv.org/pdf/1904.06618v1",
    "published_date": "2019-04-14",
    "source": "arxiv",
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ]
  }
]