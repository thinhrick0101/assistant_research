[
  {
    "paper_id": "2407.11005v2",
    "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
    "authors": [
      "Robert Friel",
      "Masha Belyi",
      "Atindriyo Sanyal"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has become a standard architectural\npattern for incorporating domain-specific knowledge into user-facing chat\napplications powered by Large Language Models (LLMs). RAG systems are\ncharacterized by (1) a document retriever that queries a domain-specific corpus\nfor context information relevant to an input query, and (2) an LLM that\ngenerates a response based on the provided query and context. However,\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\nunified evaluation criteria and annotated datasets. In response, we introduce\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\nexamples. It covers five unique industry-specific domains and various RAG task\ntypes. RAGBench examples are sourced from industry corpora such as user\nmanuals, making it particularly relevant for industry applications. Further, we\nformalize the TRACe evaluation framework: a set of explainable and actionable\nRAG evaluation metrics applicable across all RAG domains. We release the\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\nenabling actionable feedback for continuous improvement of production\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\nRAG evaluation task. We identify areas where existing approaches fall short and\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\nevaluation systems.",
    "url": "http://arxiv.org/abs/2407.11005v2",
    "pdf_url": "http://arxiv.org/pdf/2407.11005v2",
    "published_date": "2024-06-25",
    "source": "arxiv",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "paper_id": "2407.16833v2",
    "title": "Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach",
    "authors": [
      "Zhuowan Li",
      "Cheng Li",
      "Mingyang Zhang",
      "Qiaozhu Mei",
      "Michael Bendersky"
    ],
    "abstract": "Retrieval Augmented Generation (RAG) has been a powerful tool for Large\nLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,\nrecent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct a comprehensive comparison\nbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths of\nboth. We benchmark RAG and LC across various public datasets using three latest\nLLMs. Results reveal that when resourced sufficiently, LC consistently\noutperforms RAG in terms of average performance. However, RAG's significantly\nlower cost remains a distinct advantage. Based on this observation, we propose\nSelf-Route, a simple yet effective method that routes queries to RAG or LC\nbased on model self-reflection. Self-Route significantly reduces the\ncomputation cost while maintaining a comparable performance to LC. Our findings\nprovide a guideline for long-context applications of LLMs using RAG and LC.",
    "url": "http://arxiv.org/abs/2407.16833v2",
    "pdf_url": "http://arxiv.org/pdf/2407.16833v2",
    "published_date": "2024-07-23",
    "source": "arxiv",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "paper_id": "2405.07437v2",
    "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
    "authors": [
      "Hao Yu",
      "Aoran Gan",
      "Kai Zhang",
      "Shiwei Tong",
      "Qi Liu",
      "Zhaofeng Liu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has recently gained traction in natural\nlanguage processing. Numerous studies and real-world applications are\nleveraging its ability to enhance generative models through external\ninformation retrieval. Evaluating these RAG systems, however, poses unique\nchallenges due to their hybrid structure and reliance on dynamic knowledge\nsources. To better understand these challenges, we conduct A Unified Evaluation\nProcess of RAG (Auepora) and aim to provide a comprehensive overview of the\nevaluation and benchmarks of RAG systems. Specifically, we examine and compare\nseveral quantifiable metrics of the Retrieval and Generation components, such\nas relevance, accuracy, and faithfulness, within the current RAG benchmarks,\nencompassing the possible output and ground truth pairs. We then analyze the\nvarious datasets and metrics, discuss the limitations of current benchmarks,\nand suggest potential directions to advance the field of RAG benchmarks.",
    "url": "http://arxiv.org/abs/2405.07437v2",
    "pdf_url": "http://arxiv.org/pdf/2405.07437v2",
    "published_date": "2024-05-13",
    "source": "arxiv",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "paper_id": "2412.02592v1",
    "title": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation",
    "authors": [
      "Junyuan Zhang",
      "Qintong Zhang",
      "Bin Wang",
      "Linke Ouyang",
      "Zichen Wen",
      "Ying Li",
      "Ka-Ho Chow",
      "Conghui He",
      "Wentao Zhang"
    ],
    "abstract": "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge to reduce hallucinations and incorporate\nup-to-date information without retraining. As an essential part of RAG,\nexternal knowledge bases are commonly built by extracting structured data from\nunstructured PDF documents using Optical Character Recognition (OCR). However,\ngiven the imperfect prediction of OCR and the inherent non-uniform\nrepresentation of structured data, knowledge bases inevitably contain various\nOCR noises. In this paper, we introduce OHRBench, the first benchmark for\nunderstanding the cascading impact of OCR on RAG systems. OHRBench includes 350\ncarefully selected unstructured PDF documents from six real-world RAG\napplication domains, along with Q&As derived from multimodal elements in\ndocuments, challenging existing OCR solutions used for RAG To better understand\nOCR's impact on RAG systems, we identify two primary types of OCR noise:\nSemantic Noise and Formatting Noise and apply perturbation to generate a set of\nstructured data with varying degrees of each OCR noise. Using OHRBench, we\nfirst conduct a comprehensive evaluation of current OCR solutions and reveal\nthat none is competent for constructing high-quality knowledge bases for RAG\nsystems. We then systematically evaluate the impact of these two noise types\nand demonstrate the vulnerability of RAG systems. Furthermore, we discuss the\npotential of employing Vision-Language Models (VLMs) without OCR in RAG\nsystems. Code: https://github.com/opendatalab/OHR-Bench",
    "url": "http://arxiv.org/abs/2412.02592v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02592v1",
    "published_date": "2024-12-03",
    "source": "arxiv",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "paper_id": "2502.13465v1",
    "title": "HawkBench: Investigating Resilience of RAG Methods on Stratified Information-Seeking Tasks",
    "authors": [
      "Hongjin Qian",
      "Zheng Liu",
      "Chao Gao",
      "Yankai Wang",
      "Defu Lian",
      "Zhicheng Dou"
    ],
    "abstract": "In real-world information-seeking scenarios, users have dynamic and diverse\nneeds, requiring RAG systems to demonstrate adaptable resilience. To\ncomprehensively evaluate the resilience of current RAG methods, we introduce\nHawkBench, a human-labeled, multi-domain benchmark designed to rigorously\nassess RAG performance across categorized task types. By stratifying tasks\nbased on information-seeking behaviors, HawkBench provides a systematic\nevaluation of how well RAG systems adapt to diverse user needs.\n  Unlike existing benchmarks, which focus primarily on specific task types\n(mostly factoid queries) and rely on varying knowledge bases, HawkBench offers:\n(1) systematic task stratification to cover a broad range of query types,\nincluding both factoid and rationale queries, (2) integration of multi-domain\ncorpora across all task types to mitigate corpus bias, and (3) rigorous\nannotation for high-quality evaluation.\n  HawkBench includes 1,600 high-quality test samples, evenly distributed across\ndomains and task types. Using this benchmark, we evaluate representative RAG\nmethods, analyzing their performance in terms of answer quality and response\nlatency. Our findings highlight the need for dynamic task strategies that\nintegrate decision-making, query interpretation, and global knowledge\nunderstanding to improve RAG generalizability. We believe HawkBench serves as a\npivotal benchmark for advancing the resilience of RAG methods and their ability\nto achieve general-purpose information seeking.",
    "url": "http://arxiv.org/abs/2502.13465v1",
    "pdf_url": "http://arxiv.org/pdf/2502.13465v1",
    "published_date": "2025-02-19",
    "source": "arxiv",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "paper_id": "2401.17043v3",
    "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
    "authors": [
      "Yuanjie Lyu",
      "Zhiyu Li",
      "Simin Niu",
      "Feiyu Xiong",
      "Bo Tang",
      "Wenjin Wang",
      "Hao Wu",
      "Huanyong Liu",
      "Tong Xu",
      "Enhong Chen"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) is a technique that enhances the\ncapabilities of large language models (LLMs) by incorporating external\nknowledge sources. This method addresses common LLM limitations, including\noutdated information and the tendency to produce inaccurate \"hallucinated\"\ncontent. However, the evaluation of RAG systems is challenging, as existing\nbenchmarks are limited in scope and diversity. Most of the current benchmarks\npredominantly assess question-answering applications, overlooking the broader\nspectrum of situations where RAG could prove advantageous. Moreover, they only\nevaluate the performance of the LLM component of the RAG pipeline in the\nexperiments, and neglect the influence of the retrieval component and the\nexternal knowledge database. To address these issues, this paper constructs a\nlarge-scale and more comprehensive benchmark, and evaluates all the components\nof RAG systems in various RAG application scenarios. Specifically, we have\ncategorized the range of RAG applications into four distinct types-Create,\nRead, Update, and Delete (CRUD), each representing a unique use case. \"Create\"\nrefers to scenarios requiring the generation of original, varied content.\n\"Read\" involves responding to intricate questions in knowledge-intensive\nsituations. \"Update\" focuses on revising and rectifying inaccuracies or\ninconsistencies in pre-existing texts. \"Delete\" pertains to the task of\nsummarizing extensive texts into more concise forms. For each of these CRUD\ncategories, we have developed comprehensive datasets to evaluate the\nperformance of RAG systems. We also analyze the effects of various components\nof the RAG system, such as the retriever, the context length, the knowledge\nbase construction, and the LLM. Finally, we provide useful insights for\noptimizing the RAG technology for different scenarios.",
    "url": "http://arxiv.org/abs/2401.17043v3",
    "pdf_url": "http://arxiv.org/pdf/2401.17043v3",
    "published_date": "2024-01-30",
    "source": "arxiv",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "paper_id": "2412.13018v2",
    "title": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain",
    "authors": [
      "Shuting Wang",
      "Jiejun Tan",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ],
    "abstract": "As a typical and practical application of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) techniques have gained extensive\nattention, particularly in vertical domains where LLMs may lack domain-specific\nknowledge. In this paper, we introduce an omnidirectional and automatic RAG\nbenchmark, OmniEval, in the financial domain. Our benchmark is characterized by\nits multi-dimensional evaluation framework, including (1) a matrix-based RAG\nscenario evaluation system that categorizes queries into five task classes and\n16 financial topics, leading to a structured assessment of diverse query\nscenarios; (2) a multi-dimensional evaluation data generation approach, which\ncombines GPT-4-based automatic generation and human annotation, achieving an\n87.47\\% acceptance ratio in human evaluations on generated instances; (3) a\nmulti-stage evaluation system that evaluates both retrieval and generation\nperformance, result in a comprehensive evaluation on the RAG pipeline; and (4)\nrobust evaluation metrics derived from rule-based and LLM-based ones, enhancing\nthe reliability of assessments through manual annotations and supervised\nfine-tuning of an LLM evaluator. Our experiments demonstrate the\ncomprehensiveness of OmniEval, which includes extensive test datasets and\nhighlights the performance variations of RAG systems across diverse topics and\ntasks, revealing significant opportunities for RAG models to improve their\ncapabilities in vertical domains. We open source the code of our benchmark in\n\\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.",
    "url": "http://arxiv.org/abs/2412.13018v2",
    "pdf_url": "http://arxiv.org/pdf/2412.13018v2",
    "published_date": "2024-12-17",
    "source": "arxiv",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "paper_id": "2409.15763v2",
    "title": "IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through Semantic Comprehension in Retrieval-Augmented Generation Scenarios",
    "authors": [
      "Hai Lin",
      "Shaoxiong Zhan",
      "Junyou Su",
      "Haitao Zheng",
      "Hui Wang"
    ],
    "abstract": "In Retrieval-Augmented Generation (RAG) tasks using Large Language Models\n(LLMs), the quality of retrieved information is critical to the final output.\nThis paper introduces the IRSC benchmark for evaluating the performance of\nembedding models in multilingual RAG tasks. The benchmark encompasses five\nretrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval,\nkeyword retrieval, and summary retrieval. Our research addresses the current\nlack of comprehensive testing and effective comparison methods for embedding\nmodels in RAG scenarios. We introduced new metrics: the Similarity of Semantic\nComprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI),\nand evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our\ncontributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and\n3) insights into the cross-lingual limitations of embedding models. The IRSC\nbenchmark aims to enhance the understanding and development of accurate\nretrieval systems in RAG tasks. All code and datasets are available at:\nhttps://github.com/Jasaxion/IRSC_Benchmark",
    "url": "http://arxiv.org/abs/2409.15763v2",
    "pdf_url": "http://arxiv.org/pdf/2409.15763v2",
    "published_date": "2024-09-24",
    "source": "arxiv",
    "categories": [
      "cs.IR",
      "cs.AI"
    ]
  },
  {
    "paper_id": "2501.13264v2",
    "title": "RAG-Reward: Optimizing RAG with Reward Modeling and RLHF",
    "authors": [
      "Hanning Zhang",
      "Juntong Song",
      "Juno Zhu",
      "Yuanhao Wu",
      "Tong Zhang",
      "Cheng Niu"
    ],
    "abstract": "Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs)\nwith relevant and up-to-date knowledge, improving their ability to answer\nknowledge-intensive questions. It has been shown to enhance both generation\nquality and trustworthiness. While numerous works have focused on improving\nretrieval, generation, and evaluation, the role of reward models in\nreinforcement learning for optimizing RAG remains underexplored. In this paper,\nwe introduce \\textbf{RAG-Reward}, a framework designed to develop reward models\nto enable \\textit{hallucination-free, comprehensive, reliable, and efficient\nRAG}. We define four key metrics to assess generation quality and develop an\nautomated benchmarking pipeline to evaluate the outputs of multiple LLMs across\na variety of RAG scenarios. Using \\textbf{RAG-Reward}, we train reward models\nand apply {reinforcement learning with human feedback (RLHF)} to improve LLMs'\neffectiveness in RAG. Experimental results demonstrate that our reward model\nachieves state-of-the-art performance in automatic benchmarking and aligns\nclosely with human evaluations. Furthermore, the improved generation quality of\nthe trained policy model highlights the feasibility and efficiency of using\nRLHF to enhance RAG outputs.",
    "url": "http://arxiv.org/abs/2501.13264v2",
    "pdf_url": "http://arxiv.org/pdf/2501.13264v2",
    "published_date": "2025-01-22",
    "source": "arxiv",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "paper_id": "2410.12248v1",
    "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
    "authors": [
      "Jintao Liu",
      "Ruixue Ding",
      "Linhao Zhang",
      "Pengjun Xie",
      "Fie Huang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) aims to enhance large language models\n(LLMs) to generate more accurate and reliable answers with the help of the\nretrieved context from external knowledge sources, thereby reducing the\nincidence of hallucinations. Despite the advancements, evaluating these systems\nremains a crucial research area due to the following issues: (1) Limited data\ndiversity: The insufficient diversity of knowledge sources and query types\nconstrains the applicability of RAG systems; (2) Obscure problems location:\nExisting evaluation methods have difficulty in locating the stage of the RAG\npipeline where problems occur; (3) Unstable retrieval evaluation: These methods\noften fail to effectively assess retrieval performance, particularly when the\nchunking strategy changes. To tackle these challenges, we propose a\nComprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough\nevaluation across the entire RAG pipeline, including chunking, retrieval,\nreranking, and generation. To effectively evaluate the first three phases, we\nintroduce multi-granularity keywords, including coarse-grained and fine-grained\nkeywords, to assess the retrieved context instead of relying on the annotation\nof golden chunks. Moreover, we release a holistic benchmark dataset tailored\nfor diverse data scenarios covering a wide range of document formats and query\ntypes. We demonstrate the utility of the CoFE-RAG framework by conducting\nexperiments to evaluate each stage of RAG systems. Our evaluation method\nprovides unique insights into the effectiveness of RAG systems in handling\ndiverse data scenarios, offering a more nuanced understanding of their\ncapabilities and limitations.",
    "url": "http://arxiv.org/abs/2410.12248v1",
    "pdf_url": "http://arxiv.org/pdf/2410.12248v1",
    "published_date": "2024-10-16",
    "source": "arxiv",
    "categories": [
      "cs.CL"
    ]
  }
]